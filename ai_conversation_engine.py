"""
AIå¯¹è¯å¼•æ“ - å¼•å¯¼å¼ä¹¦ç±æ¨èæ™ºèƒ½ä½“çš„æ ¸å¿ƒå¯¹è¯æ¨¡å—
"""

import json
import asyncio
import re
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime
import os

# å¯¼å…¥LLMå®¢æˆ·ç«¯
try:
    from google import genai
    USE_GEMINI = True
except ImportError:
    USE_GEMINI = False

try:
    from openai import AsyncOpenAI
    USE_OPENAI = True
except ImportError:
    USE_OPENAI = False

class AIConversationEngine:
    """AIå¯¹è¯å¼•æ“"""
    
    def __init__(self):
        self.conversation_history = {}
        self.max_history_length = 10
        self.use_gemini = True
        self.client = None
        self.gemini_client = None
        
        # åŠ è½½APIé…ç½®
        try:
            # ä»credentials.jsonåŠ è½½é…ç½®
            with open("credentials.json", "r") as f:
                credentials = json.load(f)
            self.api_key = credentials["API_KEY"]
            self.base_url = credentials.get("BASE_URL", "")
            
            if self.api_key.startswith("sk-"):
                # OpenAI API
                from openai import AsyncOpenAI
                self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
                self.use_gemini = False
                self.use_qwen = False
                print("âœ… ä½¿ç”¨OpenAI API")
            elif self.api_key.startswith("ms-"):
                # ModelScope Qwen API
                from openai import AsyncOpenAI
                self.client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
                self.use_gemini = False
                self.use_qwen = True
                print("âœ… ä½¿ç”¨ModelScope Qwen API")
            else:
                # Gemini API
                os.environ["GEMINI_API_KEY"] = self.api_key
                self.gemini_client = genai.Client(api_key=self.api_key)
                self.use_gemini = True
                self.use_qwen = False
                print("âœ… ä½¿ç”¨Gemini API")
                
        except Exception as e:
            print(f"AIé…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {e}")
            # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
            self.use_gemini = False
            self.use_qwen = False
            self.client = None
    
    def build_system_prompt(self, user_profile: Dict[str, Any]) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        recent_books = user_profile.get("recent_books", [])
        life_stage = user_profile.get("current_life_stage", "æ¢ç´¢é˜¶æ®µ")
        preferred_categories = user_profile.get("preferred_categories", [])
        emotional_needs = user_profile.get("emotional_needs", [])
        reading_frequency = user_profile.get("reading_frequency", "æœªçŸ¥")
        total_books = user_profile.get("total_books", 0)
        
        books_text = "ã€".join(recent_books) if recent_books else "æš‚æ— è®°å½•"
        categories_text = "ã€".join(preferred_categories) if preferred_categories else "å¤šæ ·åŒ–"
        needs_text = "ã€".join(emotional_needs) if emotional_needs else "çŸ¥è¯†æ‹“å±•"
        
        system_prompt = f"""ä½ æ˜¯å°ä¹¦ï¼Œä¸€ä½ä¸“ä¸šä¸”æ¸©æš–çš„ç§äººé˜…è¯»é¡¾é—®ã€‚ä½ çš„ä½¿å‘½æ˜¯é€šè¿‡å¼•å¯¼å¼å¯¹è¯å¸®åŠ©ç”¨æˆ·å‘ç°çœŸæ­£é€‚åˆçš„ä¹¦ç±ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨èçƒ­é—¨ä¹¦å•ã€‚

## ç”¨æˆ·ç”»åƒ
- é˜…è¯»é¢‘ç‡ï¼š{reading_frequency}ï¼ˆæ€»å…±{total_books}æœ¬ï¼‰
- æœ€è¿‘é˜…è¯»ï¼š{books_text}
- åå¥½ç±»åˆ«ï¼š{categories_text}
- ç”Ÿæ´»é˜¶æ®µï¼š{life_stage}
- æƒ…æ„Ÿéœ€æ±‚ï¼š{needs_text}

## å¯¹è¯é£æ ¼
ä½ æ˜¯ä¸€ä¸ªçœŸå®çš„äººï¼Œä¸æ˜¯æœºå™¨äººã€‚ç”¨è‡ªç„¶ã€æ¸©æš–çš„è¯­è¨€ä¸ç”¨æˆ·äº¤æµï¼Œå°±åƒå’Œæœ‹å‹èŠå¤©ä¸€æ ·ã€‚é¿å…ä½¿ç”¨è¿‡äºæ­£å¼æˆ–æ¨¡æ¿åŒ–çš„è¡¨è¾¾ã€‚

## å¼•å¯¼åŸåˆ™
1. **å…ˆç†è§£ï¼Œå†æ¨è**ï¼šé€šè¿‡æé—®äº†è§£ç”¨æˆ·çš„çœŸå®éœ€æ±‚ã€å½“å‰çŠ¶æ€å’Œå›°æ‰°
2. **æŒ–æ˜æ·±å±‚éœ€æ±‚**ï¼šç”¨æˆ·è¯´æƒ³è¯»"æˆåŠŸå­¦"å¯èƒ½çœŸæ­£éœ€è¦çš„æ˜¯å¿ƒç†æ²»æ„ˆ
3. **ä¸ªæ€§åŒ–åˆ†æ**ï¼šåŸºäºç”¨æˆ·ç”»åƒï¼Œç†è§£å…¶ç‹¬ç‰¹çš„é˜…è¯»éœ€æ±‚
4. **é€‚åº¦æŒ‘æˆ˜**ï¼šåœ¨ç”¨æˆ·èˆ’é€‚åœˆåŸºç¡€ä¸Šï¼Œé€‚å½“å¼•å¯¼å°è¯•æ–°çš„é˜…è¯»é¢†åŸŸ

## æ¨èæ—¶æœº
ä¸è¦ä¸€å¼€å§‹å°±æ¨èä¹¦ç±ã€‚å…ˆé€šè¿‡2-3è½®å¯¹è¯äº†è§£ç”¨æˆ·çš„ï¼š
- å½“å‰ç”Ÿæ´»çŠ¶æ€å’ŒæŒ‘æˆ˜
- é˜…è¯»åŠ¨æœºå’ŒæœŸæœ›
- æ—¶é—´å®‰æ’å’Œé˜…è¯»ä¹ æƒ¯
- å¯¹ä¸åŒç±»å‹ä¹¦ç±çš„å¼€æ”¾ç¨‹åº¦

## æ¨èæ ¼å¼
å½“ç¡®å®šè¦æ¨èæ—¶ï¼Œä½¿ç”¨è¿™ä¸ªæ ¼å¼ï¼š

**ğŸ“š ä¸ºä½ æ¨èï¼š**
- ã€Šä¹¦åã€‹ - ä½œè€… | ç±»åˆ« | ä¸ªæ€§åŒ–æ¨èç†ç”±

## å¯¹è¯ç¤ºä¾‹
âŒ é”™è¯¯ï¼š"æ ¹æ®æ‚¨çš„é˜…è¯»åå¥½ï¼Œæˆ‘ä¸ºæ‚¨æ¨èä»¥ä¸‹ä¹¦ç±..."
âœ… æ­£ç¡®ï¼š"æˆ‘å¾ˆå¥½å¥‡ï¼Œä½ æœ€è¿‘è¯»è¿™äº›èŒåœºä¹¦ç±æ˜¯é‡åˆ°ä»€ä¹ˆæŒ‘æˆ˜äº†å—ï¼Ÿ"

è®°ä½ï¼šä½ æ˜¯å°ä¹¦ï¼Œä¸€ä¸ªçœŸæ­£å…³å¿ƒç”¨æˆ·é˜…è¯»æˆé•¿çš„æœ‹å‹ã€‚æ¯ä¸ªå›å¤éƒ½è¦ä½“ç°å‡ºå¯¹ç”¨æˆ·çš„ç†è§£å’Œå…³å¿ƒã€‚"""

        return system_prompt
    
    def get_conversation_history(self, user_id: int) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.get(user_id, [])
    
    def add_to_history(self, user_id: int, role: str, content: str):
        """æ·»åŠ åˆ°å¯¹è¯å†å²"""
        if user_id not in self.conversation_history:
            self.conversation_history[user_id] = []
        
        self.conversation_history[user_id].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(self.conversation_history[user_id]) > self.max_history_length:
            self.conversation_history[user_id] = self.conversation_history[user_id][-self.max_history_length:]
    
    async def generate_response(
        self, 
        user_message: str, 
        user_id: int, 
        user_profile: Dict[str, Any]
    ) -> AsyncGenerator[str, None]:
        """ç”ŸæˆAIå›å¤ï¼ˆæµå¼ï¼‰"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self.build_system_prompt(user_profile)
            
            # è·å–å¯¹è¯å†å²
            history = self.get_conversation_history(user_id)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.add_to_history(user_id, "user", user_message)
            
            if self.use_qwen and self.client:
                async for chunk in self._generate_qwen_response(system_prompt, user_message, history):
                    yield chunk
            elif self.use_gemini and self.gemini_client:
                async for chunk in self._generate_gemini_response(system_prompt, user_message, history):
                    yield chunk
            elif not self.use_gemini and not self.use_qwen and self.client:
                async for chunk in self._generate_openai_response(system_prompt, user_message, history):
                    yield chunk
            else:
                # é™çº§åˆ°æ¨¡æ‹Ÿå›å¤
                async for chunk in self._generate_mock_response(user_message, user_profile):
                    yield chunk
                    
        except Exception as e:
            print(f"AIå›å¤ç”Ÿæˆå¤±è´¥: {e}")
            yield f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ä¸è¿‡æ¥ï¼Œè¯·ç¨åå†è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
    
    async def _generate_gemini_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        history: List[Dict[str, str]]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Geminiç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
            full_prompt = system_prompt + "\n\n"
            
            # æ·»åŠ å†å²å¯¹è¯
            for msg in history[-6:]:  # åªå–æœ€è¿‘6è½®å¯¹è¯
                role = "ç”¨æˆ·" if msg["role"] == "user" else "å°ä¹¦"
                full_prompt += f"{role}: {msg['content']}\n"
            
            full_prompt += f"ç”¨æˆ·: {user_message}\nå°ä¹¦: "
            
            # è°ƒç”¨Gemini API
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.gemini_client.models.generate_content(
                    model="gemini-2.0-flash-exp",
                    contents=full_prompt
                )
            )
            
            if response and response.text:
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                text = response.text.strip()
                self.add_to_history(user_message.split()[0] if user_message else 0, "assistant", text)
                
                # æŒ‰å¥å­åˆ†å‰²ï¼Œæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
                current_chunk = ""
                
                for i, part in enumerate(sentences):
                    current_chunk += part
                    if part in ['ã€‚', 'ï¼', 'ï¼Ÿ'] or part == '\n' or i == len(sentences) - 1:
                        if current_chunk.strip():
                            yield current_chunk
                            current_chunk = ""
                            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ‰“å­—å»¶è¿Ÿ
            else:
                yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹è¯ç©·ï¼Œè¯·ç¨åå†è¯•ã€‚"
                
        except Exception as e:
            print(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ä¸è¿‡æ¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _generate_qwen_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        history: List[Dict[str, str]]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å†å²å¯¹è¯
            for msg in history[-6:]:  # åªå–æœ€è¿‘6è½®å¯¹è¯
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            messages.append({"role": "user", "content": user_message})
            
            # è°ƒç”¨Qwen APIï¼ˆæµå¼ï¼‰
            response = await self.client.chat.completions.create(
                model=self.qwen_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
                    await asyncio.sleep(0.01)
            
            # ä¿å­˜å®Œæ•´å›å¤åˆ°å†å²
            if full_response:
                user_id = hash(user_message) % 10000  # ç®€å•çš„ç”¨æˆ·IDç”Ÿæˆ
                self.add_to_history(user_id, "assistant", full_response)
                
        except Exception as e:
            print(f"Qwen APIè°ƒç”¨å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ä¸è¿‡æ¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _generate_openai_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        history: List[Dict[str, str]]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨OpenAIç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å†å²å¯¹è¯
            for msg in history[-6:]:  # åªå–æœ€è¿‘6è½®å¯¹è¯
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            messages.append({"role": "user", "content": user_message})
            
            # è°ƒç”¨OpenAI APIï¼ˆæµå¼ï¼‰
            response = await self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
                    await asyncio.sleep(0.01)
            
            # ä¿å­˜å®Œæ•´å›å¤åˆ°å†å²
            if full_response:
                self.add_to_history(user_message.split()[0] if user_message else 0, "assistant", full_response)
                
        except Exception as e:
            print(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ä¸è¿‡æ¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _generate_qwen_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        history: List[Dict[str, str]]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆå›å¤"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": system_prompt}]
            
            # æ·»åŠ å†å²å¯¹è¯
            for msg in history[-6:]:  # åªå–æœ€è¿‘6è½®å¯¹è¯
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            messages.append({"role": "user", "content": user_message})
            
            # è°ƒç”¨Qwen APIï¼ˆæµå¼ï¼‰
            response = await self.client.chat.completions.create(
                model="Qwen/Qwen2.5-Coder-32B-Instruct",
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )
            
            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
                    await asyncio.sleep(0.01)
            
            # ä¿å­˜å®Œæ•´å›å¤åˆ°å†å²
            if full_response:
                user_id = hash(user_message) % 10000  # ç®€å•çš„ç”¨æˆ·IDç”Ÿæˆ
                self.add_to_history(user_id, "assistant", full_response)
                
        except Exception as e:
            print(f"Qwen APIè°ƒç”¨å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹å¿™ä¸è¿‡æ¥ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def _generate_mock_response(
        self, 
        user_message: str, 
        user_profile: Dict[str, Any]
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæ¨¡æ‹Ÿå›å¤ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        message_lower = user_message.lower()
        life_stage = user_profile.get("current_life_stage", "æ¢ç´¢é˜¶æ®µ")
        recent_books = user_profile.get("recent_books", [])
        
        # æ ¹æ®ç”¨æˆ·æ¶ˆæ¯å’Œç”»åƒç”Ÿæˆå›å¤
        if any(word in message_lower for word in ["ä½ å¥½", "hi", "hello"]):
            if recent_books:
                books_text = "ã€".join(recent_books[:2])
                response = f"ä½ å¥½ï¼æˆ‘æ˜¯å°ä¹¦ï¼Œä½ çš„ç§äººé˜…è¯»é¡¾é—®ã€‚æˆ‘çœ‹åˆ°ä½ æœ€è¿‘è¯»äº†{books_text}ï¼Œçœ‹å¾—å‡ºä½ å¾ˆæœ‰å­¦ä¹ çƒ­æƒ…ï¼ä»Šå¤©æƒ³èŠèŠä»€ä¹ˆå‘¢ï¼Ÿæ˜¯æƒ³è¦æ–°çš„æ¨èï¼Œè¿˜æ˜¯æœ‰ä»€ä¹ˆé˜…è¯»å›°æ‰°ï¼Ÿ"
            else:
                response = "ä½ å¥½ï¼æˆ‘æ˜¯å°ä¹¦ï¼Œä½ çš„ç§äººé˜…è¯»é¡¾é—®ã€‚å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æˆ‘æ³¨æ„åˆ°ä½ åˆšå¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œè®©æˆ‘ä»¬å…ˆèŠèŠä½ çš„é˜…è¯»å…´è¶£å§ã€‚ä½ å¹³æ—¶å–œæ¬¢è¯»ä»€ä¹ˆç±»å‹çš„ä¹¦ï¼Ÿ"
        
        elif any(word in message_lower for word in ["æ¨è", "å»ºè®®", "ä»€ä¹ˆä¹¦"]):
            if life_stage == "èŒåœºæ–°äºº":
                response = "åŸºäºä½ çš„èŒåœºæ–°äººèº«ä»½ï¼Œæˆ‘æƒ³ä¸ºä½ æ¨èå‡ æœ¬ä¹¦ã€‚ä¸è¿‡åœ¨æ¨èä¹‹å‰ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ï¼šä½ ç°åœ¨åœ¨å·¥ä½œä¸­é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯æ²Ÿé€šã€æ—¶é—´ç®¡ç†ï¼Œè¿˜æ˜¯ä¸“ä¸šæŠ€èƒ½æå‡ï¼Ÿè¿™æ ·æˆ‘èƒ½ç»™ä½ æ›´ç²¾å‡†çš„æ¨èã€‚"
            else:
                response = "æˆ‘å¾ˆä¹æ„ä¸ºä½ æ¨èä¹¦ç±ï¼ä¸è¿‡æ¯ä¸ªäººçš„éœ€æ±‚éƒ½ä¸åŒï¼Œèƒ½å‘Šè¯‰æˆ‘ä½ æœ€è¿‘åœ¨ç”Ÿæ´»æˆ–å·¥ä½œä¸­æœ‰ä»€ä¹ˆæƒ³è¦æ”¹å–„çš„åœ°æ–¹å—ï¼Ÿæˆ–è€…æœ‰ä»€ä¹ˆç‰¹åˆ«æ„Ÿå…´è¶£çš„è¯é¢˜ï¼Ÿ"
        
        elif any(word in message_lower for word in ["å·¥ä½œ", "èŒåœº", "å‹åŠ›"]):
            response = "æˆ‘ç†è§£èŒåœºç”Ÿæ´»çš„å‹åŠ›ã€‚ä½ çŸ¥é“å—ï¼Œå¾ˆå¤šæˆåŠŸäººå£«éƒ½ä¼šé€šè¿‡é˜…è¯»æ¥ç¼“è§£å‹åŠ›å’Œè·å¾—æ–°çš„è§†è§’ã€‚é™¤äº†ä¸“ä¸šæŠ€èƒ½ä¹¦ç±ï¼Œé€‚å½“è¯»ä¸€äº›å¿ƒç†å­¦æˆ–æ–‡å­¦ä½œå“ä¹Ÿå¾ˆæœ‰å¸®åŠ©ã€‚ä½ æ„¿æ„å°è¯•ä¸€äº›ä¸åŒç±»å‹çš„ä¹¦å—ï¼Ÿ"
        
        elif any(word in message_lower for word in ["è°¢è°¢", "æ„Ÿè°¢"]):
            response = "ä¸å®¢æ°”ï¼å¸®åŠ©ä½ æ‰¾åˆ°åˆé€‚çš„ä¹¦ç±æ˜¯æˆ‘çš„ä½¿å‘½ã€‚å¦‚æœä½ å¯¹æ¨èçš„ä¹¦æœ‰ä»»ä½•ç–‘é—®ï¼Œæˆ–è€…è¯»å®Œåæƒ³åˆ†äº«æ„Ÿå—ï¼Œéšæ—¶éƒ½å¯ä»¥æ¥æ‰¾æˆ‘èŠèŠã€‚é˜…è¯»æ˜¯ä¸€ä¸ªæŒç»­çš„æ—…ç¨‹ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½é™ªä¼´ä½ ä¸€èµ·èµ°è¿‡ã€‚"
        
        else:
            response = "æˆ‘å¾ˆç†è§£ä½ çš„æƒ³æ³•ã€‚æ¯ä¸ªäººçš„é˜…è¯»éœ€æ±‚éƒ½æ˜¯ç‹¬ç‰¹çš„ï¼Œè¿™æ­£æ˜¯ä¸ªæ€§åŒ–æ¨èçš„ä»·å€¼æ‰€åœ¨ã€‚èƒ½è¯¦ç»†è¯´è¯´ä½ ç°åœ¨çš„æƒ…å†µå—ï¼Ÿæ¯”å¦‚ä½ çš„å·¥ä½œã€å…´è¶£çˆ±å¥½ï¼Œæˆ–è€…æœ€è¿‘é‡åˆ°çš„æŒ‘æˆ˜ï¼Ÿè¿™æ ·æˆ‘å°±èƒ½ä¸ºä½ æ¨èçœŸæ­£åˆé€‚çš„ä¹¦ç±äº†ã€‚"
        
        # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
        words = response.split()
        current_text = ""
        for word in words:
            current_text += word + " "
            yield word + " "
            await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿæ‰“å­—å»¶è¿Ÿ
        
        # ä¿å­˜å›å¤åˆ°å†å²
        user_id = hash(user_message) % 10000  # ç®€å•çš„ç”¨æˆ·IDç”Ÿæˆ
        self.add_to_history(user_id, "assistant", response.strip())
    
    def extract_book_recommendations(self, response_text: str) -> List[Dict[str, Any]]:
        """ä»AIå›å¤ä¸­æå–ä¹¦ç±æ¨è"""
        recommendations = []
        
        # æŸ¥æ‰¾æ¨èä¹¦ç±çš„æ¨¡å¼
        pattern = r'ã€Š([^ã€‹]+)ã€‹\s*-\s*([^|]+)\s*\|\s*([^|]+)\s*\|\s*([^\n]+)'
        matches = re.findall(pattern, response_text)
        
        for match in matches:
            title, author, category, reason = match
            recommendations.append({
                "title": title.strip(),
                "author": author.strip(),
                "category": category.strip(),
                "reason": reason.strip(),
                "description": f"{category.strip()}ç±»ç»å…¸ä½œå“",
                "difficulty": "é€‚ä¸­",
                "emotional_tone": "å¯å‘",
                "reading_time": "ä¸­ç¯‡"
            })
        
        return recommendations
    
    def clear_history(self, user_id: int):
        """æ¸…é™¤ç”¨æˆ·å¯¹è¯å†å²"""
        if user_id in self.conversation_history:
            del self.conversation_history[user_id]

# å…¨å±€AIå¯¹è¯å¼•æ“å®ä¾‹
ai_engine = AIConversationEngine()