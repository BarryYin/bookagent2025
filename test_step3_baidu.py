#!/usr/bin/env python3
"""ä¸“é—¨æµ‹è¯•Step3æ—ç™½ç”Ÿæˆçš„ç™¾åº¦ERNIEæ¨¡å‹è°ƒç”¨"""

import asyncio
import json
import re
import os
import sys

# ç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•çš„é…ç½®
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from openai import AsyncOpenAI
# å¯¼å…¥appbookä¸­çš„é…ç½®
if __name__ == "__main__":
    # APIé…ç½®
    BAIDU_API_KEY = "bce-v3/ALTAK-IlAGWrpPIFAMJ3g8kbD4I/f17c0a909b891c89b0dce53d913448d86a87bad9"
    BAIDU_BASE_URL = "https://qianfan.baidubce.com/v2"
    BAIDU_MODEL = "ernie-4.5-turbo-32k"

# éªŒè¯æ ‡è®°
FALLBACK_MARKERS = [
    "æ·±å—è¯»è€…å–œçˆ±çš„ç»å…¸ä½œå“",
    "é€šè¿‡ç”ŸåŠ¨çš„æ•…äº‹æƒ…èŠ‚",
    "å…·æœ‰å¾ˆé«˜çš„æ–‡å­¦ä»·å€¼",
    "è®©æˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹",
    "ä¸€ä½å€¼å¾—å°Šæ•¬çš„ä½œå®¶"
]

# æ¨¡æ‹Ÿappbook.pyä¸­çš„æ–¹æ³•å®šä¹‰
METHODOLOGY_STYLES = {
    "dongyu_literature": """## è‘£å®‡è¾‰å¼æ–‡å­¦ä½œå“è§£è¯´é£æ ¼ï¼š

### ç‰¹è‰²ï¼š
- ä¸ªäººåŒ–ç§°å‘¼ï¼ˆ"ä½ "ï¼‰ã€æƒ…æ„ŸåŒ–è¡¨è¾¾
- å¤§é‡å¤ä»Šä¸­å¤–çš„å¼•ç”¨å¯¹æ¯”
- æ·±å±‚çš„æ–‡åŒ–å’Œä»·å€¼è§‚æ¢è®¨
- å“²ç†æ€§çš„é‡‘å¥æ€»ç»“

### è¯­è¨€ç‰¹å¾ï¼šã€æ¸©æš–ã€å…±é¸£ã€å¼•ç»æ®å…¸ã€å“²ç†æ€§ã€‘""",

    "dongyu_autobiography": """## è‘£å®‡è¾‰å¼è‡ªä¼ ä½“è§£è¯´é£æ ¼ï¼š

### ç‰¹è‰²ï¼š
- ç¬¬ä¸€äººç§°çš„å™è¿°è§†è§’å’Œå¿ƒè·¯å†ç¨‹
- é€šè¿‡çœŸå®çš„æƒ…æ„Ÿæ¸²æŸ“å’Œäººç”Ÿæ•…äº‹å¼•å‘å…±é¸£
- æ·±æŒ–äº‹ä»¶èƒŒåçš„æ·±å±‚å¿ƒç†å­¦æ„ä¹‰ä¸äººç”Ÿå“²ç†
- é‡‘å¥å’Œæ€»ç»“å¯Œå«ä»·å€¼è§‚å’Œäººç”Ÿè§‚çš„å‡å""",

    "luozhenyu_efficiency": """## ç½—æŒ¯å®‡å¼æ•ˆç‡æå‡è§£è¯´é£æ ¼ï¼š

### ç‰¹è‰²ï¼š
- ç”¨å¤§é‡çš„æ•°æ®å’Œç ”ç©¶æŠ¥å‘Šæ”¯æ’‘è§‚ç‚¹
- å¼ºè°ƒæ—¶ä»£èƒŒæ™¯å’Œç«äº‰å‹åŠ›\å¼ºè°ƒè®¤çŸ¥å‡çº§çš„é‡è¦æ€§
- ä»·å€¼è§‚å·¥å…·åŒ–å’Œæ–¹æ³•åŒ–
- ç”¨ç´§è¿«æ„Ÿè¥é€ è¡ŒåŠ¨åŠ¨æœº"""
}

async def test_step3_baidu_narration():
    """éªŒè¯Step3ç¡®å®è°ƒç”¨äº†ç™¾åº¦ERNIEæ¨¡å‹ç”Ÿæˆæ—ç™½"""

    print("ğŸ¯ ===== Step3 ç™¾åº¦ERNIEæ—ç™½ç”Ÿæˆæµ‹è¯• ===== ")
    print(f"æµ‹è¯•ç›®æ ‡: éªŒè¯æ˜¯å¦èƒ½æˆåŠŸè°ƒç”¨ {BAIDU_MODEL} ç”Ÿæˆæ—ç™½è¯")
    print()

    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    slides_data = [
        {
            "slide_number": 1,
            "title": "å¼€åœºå¼•å…¥ - é‡‘é’±å¯è’™æ•™è‚²",
            "main_content": [
                "ã€Šå°ç‹—é’±é’±ã€‹é€šè¿‡ç«¥è¯å½¢å¼ä¼ æˆåŸºç¡€ç†è´¢çŸ¥è¯†",
                "ä¸€åªä¼šè¯´è¯çš„å°ç‹—é’±é’±æˆä¸ºç†è´¢å¯¼å¸ˆ",
                "å°å¥³å­©å‰å¨…é€šè¿‡è®¾å®šç›®æ ‡å’Œè¡ŒåŠ¨è·å¾—æˆåŠŸ"
            ],
            "visual_elements": {"background": "æ¸©æš–çš„ç«¥è¯é£æ ¼"}
        },
        {
            "slide_number": 2,
            "title": "æ ¸å¿ƒè¦ä¹‰ - æ¢¦æƒ³åº”æœ‰å…·ä½“å½¢å¼",
            "main_content": [
                "ä¹¦ä¸­çš„'æ¢¦æƒ³ç›¸å†Œ'è®©æŠ½è±¡ç›®æ ‡å…·åƒåŒ–",
                "æ˜ç¡®çš„ç›®æ ‡å›¾ç‰‡åŒ–è¿‡ç¨‹å¢å¼ºå®ç°çš„åŠ¨åŠ›",
                "å‰å¨…ä¸ºæé«˜è‹±è¯­æ°´å¹³é‡‡å–çš„ç›®æ ‡è§†è§‰åŒ–è¡ŒåŠ¨"
            ],
            "visual_elements": {"chart_type": "å¯¹æ¯”å›¾è¡¨"}
        }
    ]

    book_data = {
        "book_title": "å°ç‹—é’±é’±",
        "author": "åšå¤šÂ·èˆè´¹å°”",
        "category_name": "è´¢ç»æŠ•èµ„ç±»",
        "core_summary": "ä¸€æœ¬é€šè¿‡ç«¥è¯æ•…äº‹æ•™æˆåŸºç¡€ç†è´¢å’ŒæŠ•èµ„è§‚å¿µçš„å„¿ç«¥è¯»ç‰©"
    }

    methodology = "dongyu_literature"

    # æ„å»ºStep3çš„ç³»ç»Ÿæç¤ºï¼ˆæ¥è‡ªappbook.pyä¸­çš„å†…å®¹ï¼‰
    narration_style = METHODOLOGY_STYLES.get(methodology, "")

    system_prompt = f"""åŸºäºä»¥ä¸‹PPTç”»é¢ç»“æ„å’Œä¹¦ç±æ•°æ®ï¼Œä¸ºæ¯é¡µPPTåˆ›å»ºæŒ‡å®šæ–¹æ³•è®ºé£æ ¼çš„æ·±åº¦è§£è¯´è¯ã€‚è¯·ç”Ÿæˆä¸°å¯Œã€æœ‰æ·±åº¦çš„è§£è¯´å†…å®¹ã€‚

ä¹¦ç±æ•°æ®ï¼š
{json.dumps(book_data, ensure_ascii=False, indent=2)}

PPTç”»é¢ç»“æ„ï¼š
{json.dumps(slides_data, ensure_ascii=False, indent=2)}

{narration_style}

æ¯é¡µè§£è¯´è¯åŒ…å«ä»¥ä¸‹è¯¦ç»†ç»“æ„ï¼š
- slide_number: é¡µé¢ç¼–å·
- opening: å¼€åœºç™½ï¼ˆ2-3å¥è¯ï¼Œä½“ç°æ–¹æ³•è®ºç‰¹è‰²ï¼‰
- main_narration: ä¸»è¦è§£è¯´å†…å®¹ï¼ˆ2-3åˆ†é’Ÿï¼Œå¿…é¡»åŒ…å«å¾·é²å®‡è¾‰å¼çš„å…·ä½“åˆ†æå’Œå¼•ç”¨ï¼‰
- key_emphasis: é‡ç‚¹å¼ºè°ƒçš„å†…å®¹ï¼ˆæ ¸å¿ƒè§‚ç‚¹æˆ–é‡‘å¥ï¼‰
- transition: è¿‡æ¸¡è¯­ï¼ˆè¿æ¥ä¸‹ä¸€é¡µï¼Œä¿æŒè¿è´¯æ€§ï¼‰
- tone_style: è¯­è°ƒé£æ ¼å’Œæƒ…æ„Ÿè‰²å½©

**é‡è¦è¦æ±‚ï¼š**
1. å¿…é¡»å…·ä½“åˆ°ã€Šå°ç‹—é’±é’±ã€‹è¿™æœ¬ä¹¦çš„å†…å®¹
2. ä¸¥æ ¼æŒ‰ç…§"è‘£å®‡è¾‰å¼"çš„è¡¨è¾¾é£æ ¼è¦æ±‚
3. ç»“åˆPPTçš„å…·ä½“å†…å®¹è¿›è¡Œè§£è¯´
4. main_narrationè¦åŒ…å«å…·ä½“çš„ä¾‹å­å’Œæ·±å…¥åˆ†æ
5. è¯­è¨€è¦ä½“ç°æ¸©æš–ã€å…±é¸£ã€å¼•ç»æ®å…¸ã€å“²ç†æ€§é£æ ¼
6. æ¯æ®µè§£è¯´è¦èƒ½æ”¯æ’‘2-3åˆ†é’Ÿçš„è®²è§£
7. ä¸èƒ½åªæ˜¯ç¬¼ç»Ÿåœ°ä»‹ç»

è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼ˆä¸¥æ ¼JSONï¼‰ï¼Œç¡®ä¿æ¯é¡µè§£è¯´éƒ½å®Œå…¨ç¬¦åˆè‘£å®‡è¾‰é£æ ¼ã€‚
"""

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        print("ğŸ¤– åˆå§‹åŒ–ç™¾åº¦ERNIEå®¢æˆ·ç«¯...")
        client = AsyncOpenAI(api_key=BAIDU_API_KEY, base_url=BAIDU_BASE_URL)

        # ç›´æ¥è°ƒç”¨APIï¼ˆæ¨¡æ‹Ÿå®é™…çš„Step3è¿‡ç¨‹ï¼‰
        print(f"\nğŸ“¡ è°ƒç”¨ç™¾åº¦ERNIEæ¨¡å‹: {BAIDU_MODEL}")
        print("ğŸ’¬ ç³»ç»Ÿæç¤ºé•¿åº¦:", len(system_prompt))

        start_time = asyncio.get_event_loop().time()

        response = await client.chat.completions.create(
            model=BAIDU_MODEL,
            messages=[{"role": "user", "content": system_prompt}],
            temperature=0.8,
            max_tokens=2000
        )

        elapsed = asyncio.get_event_loop().time() - start_time

        # è·å–å“åº”
        result = response.choices[0].message.content

        print("\n" + "="*60)
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ!")
        print(f"ğŸ“Š å“åº”æ—¶é—´: {elapsed:.2f}ç§’")
        print(f"ğŸ“„ æ€»é•¿åº¦: {len(result)}å­—ç¬¦")
        print(f"ğŸ†” å“åº”ID: {response.id}")
        print(f"ğŸ¤– ä½¿ç”¨çš„æ¨¡å‹: {response.model}")
        print(f"ğŸ’° ä»¤ç‰Œç»Ÿè®¡: {response.usage}")
        print("="*60)

        # éªŒè¯å†…å®¹è´¨é‡
        print("\nğŸ” å†…å®¹éªŒè¯:")

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¤‡ç”¨æ¨¡æ¿å†…å®¹
        has_fallback = any(marker in result for marker in FALLBACK_MARKERS)
        if has_fallback:
            print("âš ï¸ è­¦å‘Š: åŒ…å«å¤‡ç”¨æ¨¡æ¿å†…å®¹å…³é”®è¯")
        else:
            print("âœ… å†…å®¹çœ‹èµ·æ¥æ˜¯åŠ¨æ€ç”Ÿæˆçš„")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“ç»†èŠ‚
        specific_terms = ["åšå¤š", "èˆè´¹å°”", "æ¢¦æƒ³ç›¸å†Œ", "é’±é’±", "å‚¨è“„ç½", "ç†è´¢å¯è’™"]
        found_terms = [term for term in specific_terms if term in result]
        print(f"âœ… æ‰¾åˆ° {len(found_terms)} ä¸ªã€Šå°ç‹—é’±é’±ã€‹ç‹¬æœ‰å…ƒç´ : {found_terms}")

        # æ£€æŸ¥è‘£å®‡è¾‰é£æ ¼ç‰¹å¾
        style_features = {
            "ç¬¬ä¸€äººç§°": "ä½ " in result or "æˆ‘ä»¬" in result,
            "æƒ…æ„ŸåŒ–": any(word in result for word in ["æ¸©æš–", "æ„ŸåŠ¨", "å…±é¸£"]),
            "å¤å…¸å¼•ç”¨": any(text in result for text in ["ã€Šè®ºè¯­ã€‹", "å­Ÿå­", "è€å­"]) or any(text in result for text in ["ç‰µç‰›ç»‡å¥³", "è¯—ç»", "è®ºè¯­"]),
            "æ•…äº‹å¼•ç”¨": "æ•…äº‹" in result,
            "å“²ç†è¡¨è¾¾": any(phrase in result for phrase in ["æ­£å¦‚", "å¤äºº", "æ™ºæ…§", "äººç”Ÿ"])
        }

        print("\nğŸ¨ é£æ ¼éªŒè¯:")
        for feature, found in style_features.items():
            status = "âœ…" if found else "âŒ"
            print(f"   {status} {feature}")

        # è¾“å‡ºå“åº”æ ·æœ¬
        print(f"\nğŸ“ å“åº”æ ·æœ¬ï¼ˆå‰400å­—ç¬¦ï¼‰:\n{result[:400]}...")

        # å°è¯•JSONè§£æ
        try:
            if "```json" in result:
                json_match = re.search(r'```json\s*\n(.*?)\n```', result, re.DOTALL, re.DOTALL)
                if json_match:
                    narrations = json.loads(json_match.group(1))
                else:
                    json_match = re.search(r'\[(.*?)\]', result, re.DOTALL)
                    narrations = json.loads('[' + json_match.group(1) + ']')
            else:
                # å°è¯•ä»æ•°ç»„æ‹¬å·åŒ¹é…
                narrations = json.loads(result)

            print(f"\nâœ… JSONè§£ææˆåŠŸ!")
            print(f"ğŸ“Š æ—ç™½æ•°é‡: {len(narrations)}")

            # è¯¦ç»†å±•ç¤ºæ¯ä¸ªæ—ç™½
            for i, narration in enumerate(narrations[:2], 1):
                print(f"\nğŸ¬ ç¬¬{i}é¡µæ—ç™½:")
                print(f"   å¼€åœº: {narration.get('opening', 'æ— ')[:60]}...")
                print(f"   é•¿åº¦: {len(narration.get('main_narration', ''))}å­—ç¬¦")
                print(f"   é£æ ¼: {narration.get('tone_style', 'æœªæ ‡æ˜')}")

        except Exception as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print("åŸå§‹å“åº”æ ¼å¼:", result[:200])

        print("\n" + "="*60)
        print("ğŸ‰ æµ‹è¯•ç»“è®º:")

        # è¯„ä¼°ç»“è®º
        api_success = len(found_terms) >= 2
        quality_score = sum(style_features.values()) / len(style_features)

        if api_success and quality_score >= 0.5:
            print("âœ… SUCCESS: ç™¾åº¦ERNIEæ¨¡å‹è°ƒç”¨æˆåŠŸï¼Œç”Ÿæˆäº†ä¸ªæ€§åŒ–çš„æ—ç™½å†…å®¹")
        elif not api_success:
            print("âŒ FAILED: ç”Ÿæˆçš„å†…å®¹å¯èƒ½è¿‡äºé€šç”¨ï¼Œç¼ºä¹ä¸ªæ€§åŒ–ç‰¹å¾")
        else:
            print("âš ï¸ PARTIAL: è°ƒç”¨æˆåŠŸä½†é£æ ¼ç‰¹å¾ä¸æ˜æ˜¾")

    except Exception as e:
        print("\n" + "="*60)
        print("âŒ æµ‹è¯•å¤±è´¥")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")

        # å¸¸è§é”™è¯¯ç±»å‹åˆ†æ
        error_info = str(e)
        if "401" in error_info or "invalid_model" in error_info:
            print("\nğŸ” é”™è¯¯åˆ†æ:")
            print("   1. API Key å¯èƒ½å·²è¿‡æœŸæˆ–æƒé™ä¸è¶³")
            print("   2. æ¨¡å‹åç§°å¯èƒ½å­˜åœ¨æ‹¼å†™é”™è¯¯")
            print("   3. éœ€è¦æ£€æŸ¥ç™¾åº¦åƒå¸†å¹³å°çš„æ¨¡å‹å¼€é€šçŠ¶æ€")
            print("   4. å¯èƒ½éœ€è¦åˆ‡æ¢åˆ°å…¶ä»–å¯ç”¨æ¨¡å‹")

            print("\nğŸ”¨ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("   a) ç™»å½• https://console.bce.baidu.com/ æŸ¥çœ‹APIçŠ¶æ€")
            print("   b) å°è¯•ä½¿ç”¨'ernie-4.5'æ›¿ä»£'ernie-4.5-turbo-32k'")
            print("   c) æ£€æŸ¥API Keyæ˜¯å¦æœ‰è°ƒç”¨æƒé™")

if __name__ == "__main__":
    asyncio.run(test_step3_baidu_narration())